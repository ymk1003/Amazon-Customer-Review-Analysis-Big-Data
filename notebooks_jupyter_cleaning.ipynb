{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea3a277-bb33-4951-aead-60ed1fec7670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAFE META CLEANER (handles missing cols, case issues, GCS output)\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import time\n",
    "\n",
    "# --- GCS locations ---\n",
    "GCS_BASE            = \"gs://qst843-project/amazon_reviews_2023\"\n",
    "META_RAW_ROOT       = f\"{GCS_BASE}/bronze/meta_parquet\"\n",
    "META_CLEAN_ROOT     = f\"{GCS_BASE}/bronze/clean_data/meta\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.project\", \"qst843-project\")\n",
    "\n",
    "# --- helpers ---\n",
    "def col_if_exists(df, name, dtype=\"string\"):\n",
    "    \"\"\"Return column if it exists, else NULL of desired type.\"\"\"\n",
    "    return F.col(name) if name in df.columns else F.lit(None).cast(dtype)\n",
    "\n",
    "def size_if_exists(df, name):\n",
    "    \"\"\"Return size(col) if it exists else 0.\"\"\"\n",
    "    return F.size(F.col(name)) if name in df.columns else F.lit(0)\n",
    "\n",
    "# --- main cleaner ---\n",
    "def clean_existing_meta(cat, do_count=True):\n",
    "    src_path  = f\"{META_RAW_ROOT}/{cat}\"\n",
    "    dest_path = f\"{META_CLEAN_ROOT}/{cat}\"\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] START cleaning {cat}\")\n",
    "    df = spark.read.parquet(src_path)\n",
    "\n",
    "    # --- guarantee parent_asin ---\n",
    "    df = df.withColumn(\n",
    "        \"parent_asin\",\n",
    "        F.coalesce(col_if_exists(df, \"parent_asin\"), col_if_exists(df, \"asin\")).cast(\"string\")\n",
    "    )\n",
    "\n",
    "    # --- product flags ---\n",
    "    df = df.withColumn(\"product_image\", (F.coalesce(size_if_exists(df, \"images\"), F.lit(0)) > 0)) \\\n",
    "           .withColumn(\"product_video\", (F.coalesce(size_if_exists(df, \"videos\"), F.lit(0)) > 0))\n",
    "\n",
    "    # --- drop large/unneeded fields if they exist ---\n",
    "    for c in [\"images\", \"videos\", \"details\", \"_corrupt_record\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "\n",
    "    # --- trim array text fields ---\n",
    "    def trim_array(colname):\n",
    "        if colname in df.columns:\n",
    "            return F.transform(\n",
    "                F.col(colname),\n",
    "                lambda x: F.trim(F.regexp_replace(x.cast(\"string\"), r\"\\s+\", \" \"))\n",
    "            )\n",
    "        return F.lit(None).cast(\"array<string>\")\n",
    "\n",
    "    df = df.withColumn(\"features\", trim_array(\"features\")) \\\n",
    "           .withColumn(\"description\", trim_array(\"description\"))\n",
    "\n",
    "    # --- trim scalar text fields ---\n",
    "    for c in [\"title\", \"brand\", \"store\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.trim(F.regexp_replace(F.col(c).cast(\"string\"), r\"\\s+\", \" \")))\n",
    "    if \"store\" in df.columns:\n",
    "        df = df.withColumn(\"store\", F.lower(F.col(\"store\")))\n",
    "\n",
    "    # --- handle nested categories ---\n",
    "    if \"main_category\" not in df.columns and \"main_cat\" in df.columns:\n",
    "        df = df.withColumn(\"main_category\", F.col(\"main_cat\"))\n",
    "    if \"categories\" in df.columns:\n",
    "        dt = [f for f in df.schema if f.name == \"categories\"][0].dataType\n",
    "        if isinstance(dt, T.ArrayType) and isinstance(dt.elementType, T.ArrayType):\n",
    "            df = df.withColumn(\n",
    "                \"categories\", F.expr(\"transform(categories, x -> array_join(x, ' > '))\")\n",
    "            )\n",
    "\n",
    "    # --- parse price into double ---\n",
    "    if \"price\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"price\",\n",
    "            F.when(F.col(\"price\").cast(\"double\").isNotNull(), F.col(\"price\").cast(\"double\"))\n",
    "             .otherwise(F.regexp_replace(F.col(\"price\").cast(\"string\"), r\"[^0-9.\\-]\", \"\").cast(\"double\"))\n",
    "        )\n",
    "\n",
    "    # --- reorder & ensure missing columns exist ---\n",
    "    ordered_cols = [\n",
    "        (\"parent_asin\", T.StringType()),\n",
    "        (\"title\", T.StringType()),\n",
    "        (\"main_category\", T.StringType()),\n",
    "        (\"categories\", T.ArrayType(T.StringType())),\n",
    "        (\"price\", T.DoubleType()),\n",
    "        (\"features\", T.ArrayType(T.StringType())),\n",
    "        (\"description\", T.ArrayType(T.StringType())),\n",
    "        (\"average_rating\", T.DoubleType()),\n",
    "        (\"rating_number\", T.LongType()),\n",
    "        (\"brand\", T.StringType()),\n",
    "        (\"store\", T.StringType()),\n",
    "        (\"product_image\", T.BooleanType()),\n",
    "        (\"product_video\", T.BooleanType()),\n",
    "    ]\n",
    "\n",
    "    for c, t in ordered_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None).cast(t))\n",
    "    df = df.select(*[F.col(c).cast(t) for c, t in ordered_cols])\n",
    "\n",
    "    # --- write cleaned output ---\n",
    "    (df.coalesce(8)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"compression\", \"snappy\")\n",
    "        .parquet(dest_path))\n",
    "\n",
    "    if do_count:\n",
    "        print(f\"Total rows: {df.count():,}\")\n",
    "    print(f\"✅ Cleaned meta written to: {dest_path}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# clean_existing_meta(\"All_Beauty\")\n",
    "# clean_existing_meta(\"Electronics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d220d3e-3979-4c00-8f9e-bcb7f430d04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:49:44] START cleaning Toys_and_Games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 890,874\n",
      "✅ Cleaned meta written to: gs://qst843-project/amazon_reviews_2023/bronze/clean_data/meta/Toys_and_Games\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean_existing_meta(\"All_Beauty\")\n",
    "# clean_existing_meta(\"Arts_Crafts_and_Sewing\")\n",
    "# clean_existing_meta(\"Automotive\")\n",
    "# clean_existing_meta(\"Baby_Products\")\n",
    "# clean_existing_meta(\"Cell_Phones_and_Accessories\")\n",
    "# clean_existing_meta(\"Grocery_and_Gourmet_Food\")\n",
    "# clean_existing_meta(\"Health_and_Personal_Care\")\n",
    "# clean_existing_meta(\"Musical_Instruments\")\n",
    "# clean_existing_meta(\"Office_Products\")\n",
    "# clean_existing_meta(\"Patio_Lawn_and_Garden\")\n",
    "# clean_existing_meta(\"Pet_Supplies\")\n",
    "# clean_existing_meta(\"Sports_and_Outdoors\")\n",
    "# clean_existing_meta(\"Toys_and_Games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1f7a800-8813-4a83-85ad-bdf14cd06d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAFE REVIEWS CLEANER (handles missing cols, timestamp ms→ts, dedupe)\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import time\n",
    "\n",
    "# --- GCS locations ---\n",
    "GCS_BASE                 = \"gs://qst843-project/amazon_reviews_2023\"\n",
    "REVIEWS_RAW_ROOT         = f\"{GCS_BASE}/bronze/reviews_parquet_by_cat\"\n",
    "REVIEWS_CLEAN_ROOT       = f\"{GCS_BASE}/bronze/clean_data/reviews\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.project\", \"qst843-project\")\n",
    "\n",
    "# --- helpers ---\n",
    "def col_if_exists(df, name, dtype=\"string\"):\n",
    "    return F.col(name) if name in df.columns else F.lit(None).cast(dtype)\n",
    "\n",
    "def size_if_exists(df, name):\n",
    "    return F.size(F.col(name)) if name in df.columns else F.lit(0)\n",
    "\n",
    "def clean_existing_reviews(cat: str, do_count: bool = True):\n",
    "    \"\"\"\n",
    "    Read raw REVIEWS parquet for a category, clean it, and write to clean_data/reviews/<cat>.\n",
    "    Keeps originals under bronze/reviews_parquet_by_cat/<cat>.\n",
    "    \"\"\"\n",
    "    src_path  = f\"{REVIEWS_RAW_ROOT}/{cat}\"\n",
    "    dest_path = f\"{REVIEWS_CLEAN_ROOT}/{cat}\"\n",
    "\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] START reviews clean {cat}\")\n",
    "    df = spark.read.parquet(src_path)\n",
    "\n",
    "    # --- guarantee parent_asin ---\n",
    "    df = df.withColumn(\n",
    "        \"parent_asin\",\n",
    "        F.coalesce(col_if_exists(df, \"parent_asin\"), col_if_exists(df, \"asin\")).cast(\"string\")\n",
    "    )\n",
    "\n",
    "    # --- rating, helpful_vote, verified_purchase types ---\n",
    "    df = df.withColumn(\"rating\", col_if_exists(df, \"rating\", \"double\").cast(\"int\")) \\\n",
    "           .withColumn(\"helpful_vote\", col_if_exists(df, \"helpful_vote\", \"long\").cast(\"int\")) \\\n",
    "           .withColumn(\"verified_purchase\", col_if_exists(df, \"verified_purchase\", \"boolean\").cast(\"boolean\"))\n",
    "\n",
    "    # --- timestamp(ms) → timestamp ---\n",
    "    # Some dumps use 'timestamp' (ms), others 'unixReviewTime' (sec)\n",
    "    ts_ms = col_if_exists(df, \"timestamp\", \"long\")\n",
    "    ts_sec_alt = col_if_exists(df, \"unixReviewTime\", \"long\")\n",
    "    df = df.withColumn(\n",
    "            \"_ts_sec\",\n",
    "            F.when(ts_ms.isNotNull(), (ts_ms.cast(\"double\")/1000.0))\n",
    "             .otherwise(ts_sec_alt.cast(\"double\"))\n",
    "        ) \\\n",
    "        .withColumn(\"timestamp\",\n",
    "            F.to_timestamp(F.from_unixtime(F.col(\"_ts_sec\")))\n",
    "        ).drop(\"_ts_sec\")\n",
    "\n",
    "    # --- text fields tidy ---\n",
    "    for c in [\"title\",\"text\"]:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            F.trim(F.regexp_replace(col_if_exists(df, c).cast(\"string\"), r\"\\s+\", \" \"))\n",
    "        )\n",
    "\n",
    "    # --- review image flag & drop heavy arrays ---\n",
    "    df = df.withColumn(\"review_image\", (F.coalesce(size_if_exists(df, \"images\"), F.lit(0)) > 0))\n",
    "    if \"images\" in df.columns:\n",
    "        df = df.drop(\"images\")\n",
    "\n",
    "    # --- basic filters: rating in [1,5] if rating exists ---\n",
    "    if \"rating\" in df.columns:\n",
    "        df = df.where((F.col(\"rating\") >= 1) & (F.col(\"rating\") <= 5))\n",
    "\n",
    "    # --- dedupe ---\n",
    "    # key uses what we likely have: (user_id, parent_asin, timestamp, title)\n",
    "    df = df.dropDuplicates([\"user_id\", \"parent_asin\", \"timestamp\", \"title\"])\n",
    "\n",
    "    # --- final columns (ensure presence) ---\n",
    "    ordered_cols = [\n",
    "        (\"user_id\",          T.StringType()),\n",
    "        (\"parent_asin\",      T.StringType()),\n",
    "        (\"timestamp\",        T.TimestampType()),\n",
    "        (\"rating\",           T.IntegerType()),\n",
    "        (\"title\",            T.StringType()),\n",
    "        (\"text\",             T.StringType()),\n",
    "        (\"helpful_vote\",     T.IntegerType()),\n",
    "        (\"verified_purchase\",T.BooleanType()),\n",
    "        (\"review_image\",     T.BooleanType()),\n",
    "    ]\n",
    "    for c, t in ordered_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None).cast(t))\n",
    "    df = df.select(*[F.col(c).cast(t) for c, t in ordered_cols])\n",
    "\n",
    "    # --- write cleaned copy (originals preserved) ---\n",
    "    (df.coalesce(8)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .option(\"compression\",\"snappy\")\n",
    "       .parquet(dest_path))\n",
    "\n",
    "    if do_count:\n",
    "        print(f\"Total rows: {df.count():,}\")\n",
    "    print(f\"✅ Cleaned reviews written to: {dest_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcf03513-71ff-470a-bd51-a6b32ee54847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:00:32] START reviews clean Sports_and_Outdoors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"All_Beauty\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"Arts_Crafts_and_Sewing\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"Automotive\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"Patio_Lawn_and_Garden\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"Pet_Supplies\")\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mclean_existing_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSports_and_Outdoors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# clean_existing_reviews(\"Toys_and_Games\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m, in \u001b[0;36mclean_existing_reviews\u001b[0;34m(cat, do_count)\u001b[0m\n\u001b[1;32m     29\u001b[0m dest_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREVIEWS_CLEAN_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] START reviews clean \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# --- guarantee parent_asin ---\u001b[39;00m\n\u001b[1;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_asin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     F\u001b[38;5;241m.\u001b[39mcoalesce(col_if_exists(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_asin\u001b[39m\u001b[38;5;124m\"\u001b[39m), col_if_exists(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clean_existing_reviews(\"All_Beauty\")\n",
    "# clean_existing_reviews(\"Arts_Crafts_and_Sewing\")\n",
    "# clean_existing_reviews(\"Automotive\")\n",
    "# clean_existing_reviews(\"Baby_Products\")\n",
    "# clean_existing_reviews(\"Cell_Phones_and_Accessories\")\n",
    "# clean_existing_reviews(\"Grocery_and_Gourmet_Food\")\n",
    "# clean_existing_reviews(\"Health_and_Personal_Care\")\n",
    "# clean_existing_reviews(\"Musical_Instruments\")\n",
    "# clean_existing_reviews(\"Office_Products\")\n",
    "# clean_existing_reviews(\"Patio_Lawn_and_Garden\")\n",
    "\n",
    "\n",
    "# clean_existing_reviews(\"Pet_Supplies\")\n",
    "# clean_existing_reviews(\"Sports_and_Outdoors\")\n",
    "# clean_existing_reviews(\"Toys_and_Games\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a91d7a82-7a96-4078-bd89-39fb2c409a23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cleanup done.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, shutil, pathlib\n",
    "\n",
    "# Spark temp dirs\n",
    "for root in (\"/tmp\", \"/var/tmp\"):\n",
    "    for p in glob.glob(os.path.join(root, \"spark-*\")):\n",
    "        try:\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "        except: pass\n",
    "\n",
    "# Our staging\n",
    "for p in glob.glob(\"/mnt/tmp/meta_*.jsonl*\") + glob.glob(\"/mnt/tmp/review_*.jsonl*\"):\n",
    "    try: os.remove(p)\n",
    "    except: pass\n",
    "\n",
    "# Optional: wipe any previous local outputs used for testing\n",
    "for p in [\"/mnt/data/amazon2023_local\", \"/mnt/data/spark-tmp\"]:\n",
    "    if os.path.exists(p):\n",
    "        shutil.rmtree(p, ignore_errors=True)\n",
    "\n",
    "print(\"Local cleanup done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c15f13-4c9d-4dae-9cf2-51d021e4d712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:51:45] Scanning cleaned meta folders...\n",
      "  ✔ All_Beauty\n",
      "  ✔ Arts_Crafts_and_Sewing\n",
      "  ✔ Automotive\n",
      "  ✔ Baby_Products\n",
      "  ✔ Cell_Phones_and_Accessories\n",
      "  ✔ Grocery_and_Gourmet_Food\n",
      "  ✔ Health_and_Personal_Care\n",
      "  ✔ Musical_Instruments\n",
      "  ✔ Office_Products\n",
      "  ✔ Patio_Lawn_and_Garden\n",
      "[02:51:45] ► (1/10) Reading All_Beauty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:51:48] ► (2/10) Reading Arts_Crafts_and_Sewing\n",
      "[02:51:49] ► (3/10) Reading Automotive\n",
      "[02:51:49] ► (4/10) Reading Baby_Products\n",
      "[02:51:50] ► (5/10) Reading Cell_Phones_and_Accessories\n",
      "[02:51:50] ► (6/10) Reading Grocery_and_Gourmet_Food\n",
      "[02:51:51] ► (7/10) Reading Health_and_Personal_Care\n",
      "[02:51:51] ► (8/10) Reading Musical_Instruments\n",
      "[02:51:51] ► (9/10) Reading Office_Products\n",
      "[02:51:52] ► (10/10) Reading Patio_Lawn_and_Garden\n",
      "[02:51:52] Repartitioning and writing compact dataset to: gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:52:36] ✅ Done.\n",
      "Output: gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD + COMPACT CLEANED META (no precombined source needed)\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Categories you want\n",
    "META_CATS = [\n",
    "    \"All_Beauty\",\n",
    "    \"Arts_Crafts_and_Sewing\",\n",
    "    \"Automotive\",\n",
    "    \"Baby_Products\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Health_and_Personal_Care\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\",\n",
    "]\n",
    "\n",
    "GCS_BASE            = \"gs://qst843-project/amazon_reviews_2023\"\n",
    "META_CLEAN_ROOT     = f\"{GCS_BASE}/bronze/clean_data/meta\"\n",
    "DST                 = f\"{GCS_BASE}/silver/meta_combined_compact\"   # final compacted output\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.project\",\"qst843-project\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"128\")\n",
    "\n",
    "def path_exists(uri: str) -> bool:\n",
    "    try:\n",
    "        jvm = spark._jvm\n",
    "        conf = spark._jsc.hadoopConfiguration()\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI(uri), conf)\n",
    "        return fs.exists(jvm.org.apache.hadoop.fs.Path(uri))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Scanning cleaned meta folders...\")\n",
    "paths = []\n",
    "for cat in META_CATS:\n",
    "    p = f\"{META_CLEAN_ROOT}/{cat}\"\n",
    "    if path_exists(p):\n",
    "        print(f\"  ✔ {cat}\")\n",
    "        paths.append((cat, p))\n",
    "    else:\n",
    "        print(f\"  ⚠️  missing: {p}\")\n",
    "\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\"No cleaned meta folders found under bronze/clean_data/meta for the given categories.\")\n",
    "\n",
    "# Read & union with category tag (append-by-loop to avoid schema pain)\n",
    "df_all = None\n",
    "for i, (cat, p) in enumerate(paths, 1):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] ► ({i}/{len(paths)}) Reading {cat}\")\n",
    "    df_cat = (spark.read\n",
    "                 .option(\"recursiveFileLookup\",\"true\")\n",
    "                 .parquet(p)\n",
    "             ).withColumn(\"category_name\", F.lit(cat))\n",
    "    if df_all is None:\n",
    "        df_all = df_cat\n",
    "    else:\n",
    "        df_all = df_all.unionByName(df_cat, allowMissingColumns=True)\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Repartitioning and writing compact dataset to: {DST}\")\n",
    "n_files = 16  # adjust to 8/16/32 as needed\n",
    "(df_all.repartition(n_files)\n",
    "      .write.mode(\"overwrite\")\n",
    "      .option(\"compression\",\"snappy\")\n",
    "      .parquet(DST))\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] ✅ Done.\")\n",
    "print(\"Output:\", DST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34e5d18-3a91-49d4-869b-bccd6ce51fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:00:58] Scanning cleaned reviews folders...\n",
      "  ✔ All_Beauty\n",
      "  ✔ Arts_Crafts_and_Sewing\n",
      "  ✔ Automotive\n",
      "  ✔ Baby_Products\n",
      "  ✔ Cell_Phones_and_Accessories\n",
      "  ✔ Grocery_and_Gourmet_Food\n",
      "  ✔ Health_and_Personal_Care\n",
      "  ✔ Musical_Instruments\n",
      "  ✔ Office_Products\n",
      "  ✔ Patio_Lawn_and_Garden\n",
      "[03:00:58] ► (1/10) Reading All_Beauty\n",
      "[03:00:58] ► (2/10) Reading Arts_Crafts_and_Sewing\n",
      "[03:00:59] ► (3/10) Reading Automotive\n",
      "[03:00:59] ► (4/10) Reading Baby_Products\n",
      "[03:00:59] ► (5/10) Reading Cell_Phones_and_Accessories\n",
      "[03:01:00] ► (6/10) Reading Grocery_and_Gourmet_Food\n",
      "[03:01:00] ► (7/10) Reading Health_and_Personal_Care\n",
      "[03:01:00] ► (8/10) Reading Musical_Instruments\n",
      "[03:01:01] ► (9/10) Reading Office_Products\n",
      "[03:01:01] ► (10/10) Reading Patio_Lawn_and_Garden\n",
      "[03:01:01] Writing compacted reviews to: gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact as ~16 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:02:44] ✅ Done.\n",
      "Output: gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD + COMPACT CLEANED REVIEWS (no precombined source needed)\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Categories to combine (your list)\n",
    "REV_CATS = [\n",
    "    \"All_Beauty\",\n",
    "    \"Arts_Crafts_and_Sewing\",\n",
    "    \"Automotive\",\n",
    "    \"Baby_Products\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Grocery_and_Gourmet_Food\",\n",
    "    \"Health_and_Personal_Care\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio_Lawn_and_Garden\",\n",
    "]\n",
    "\n",
    "GCS_BASE              = \"gs://qst843-project/amazon_reviews_2023\"\n",
    "REVIEWS_CLEAN_ROOT    = f\"{GCS_BASE}/bronze/clean_data/reviews\"\n",
    "REV_COMBINED_COMPACT  = f\"{GCS_BASE}/silver/reviews_combined_compact\"  # final compacted output\n",
    "\n",
    "# GCS + Spark settings\n",
    "spark.conf.set(\"spark.hadoop.fs.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.project\",\"qst843-project\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"128\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\",\"true\")\n",
    "\n",
    "def path_exists(uri: str) -> bool:\n",
    "    try:\n",
    "        jvm = spark._jvm\n",
    "        conf = spark._jsc.hadoopConfiguration()\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI(uri), conf)\n",
    "        return fs.exists(jvm.org.apache.hadoop.fs.Path(uri))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Scanning cleaned reviews folders...\")\n",
    "existing = []\n",
    "for cat in REV_CATS:\n",
    "    p = f\"{REVIEWS_CLEAN_ROOT}/{cat}\"\n",
    "    if path_exists(p):\n",
    "        print(f\"  ✔ {cat}\")\n",
    "        existing.append((cat, p))\n",
    "    else:\n",
    "        print(f\"  ⚠️  missing: {p}\")\n",
    "\n",
    "if not existing:\n",
    "    raise FileNotFoundError(\"No cleaned reviews found under bronze/clean_data/reviews for the given categories.\")\n",
    "\n",
    "# Read + union incrementally (robust to schema drift)\n",
    "df_all = None\n",
    "for i, (cat, p) in enumerate(existing, 1):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] ► ({i}/{len(existing)}) Reading {cat}\")\n",
    "    df_cat = (spark.read\n",
    "                .option(\"recursiveFileLookup\",\"true\")\n",
    "                .parquet(p)\n",
    "              ).withColumn(\"category_name\", F.lit(cat))\n",
    "    if df_all is None:\n",
    "        df_all = df_cat\n",
    "    else:\n",
    "        df_all = df_all.unionByName(df_cat, allowMissingColumns=True)\n",
    "\n",
    "# Compact to a small number of large files (fast to read)\n",
    "n_files = 16   # adjust to 8/16/32 as needed (lower if disk is tight)\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Writing compacted reviews to: {REV_COMBINED_COMPACT} as ~{n_files} files\")\n",
    "(df_all.repartition(n_files)\n",
    "      .write.mode(\"overwrite\")\n",
    "      .option(\"compression\",\"snappy\")\n",
    "      .parquet(REV_COMBINED_COMPACT))\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] ✅ Done.\")\n",
    "print(\"Output:\", REV_COMBINED_COMPACT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323d95f3-62b6-41ce-84e3-7dbf8f380f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:03:44] Checking combined reviews parquet at: gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\n",
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- helpful_vote: integer (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- review_image: boolean (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Total rows (approx): 102,531,726\n",
      "\n",
      "[✓] Sample records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "|             user_id|parent_asin|          timestamp|rating|               title|       category_name|\n",
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "|AGZPNP4EC4Z7CTHY2...| B07V34XSJ8|2021-05-30 08:51:53|     5|        Comfortable!|Arts_Crafts_and_S...|\n",
      "|AFOCCQXZYCTLGLQ4Y...| B0047BITNI|2015-03-10 18:23:14|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AE5XOXRPK5ZCDD2DC...| B08Z7CRNSC|2022-01-17 16:45:32|     5|        Very pleased|Arts_Crafts_and_S...|\n",
      "|AE4JS4KHF5SU7PICZ...| B007C7XPME|2020-11-23 13:29:21|     5|        Fun and Easy|Arts_Crafts_and_S...|\n",
      "|AHZW6N77UGOLTYM6A...| B00FFFR7E2|2020-01-28 07:32:49|     5|        They are big|Arts_Crafts_and_S...|\n",
      "|AEXD6MEZ562LW7JGA...| B005R4FEKA|2018-05-26 02:03:41|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AEK5UPTJQEIOPKJNI...| B071S4747T|2019-08-08 04:24:17|     5|Great product, I ...|Arts_Crafts_and_S...|\n",
      "|AE4SU34EYNA4YQSZ7...| B01LY9ERQC|2016-10-20 15:05:25|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AHORDNMHYEIZEUCTL...| B016HS9EAK|2014-10-07 13:58:30|     4|          Four Stars|Arts_Crafts_and_S...|\n",
      "|AEBG3BZ573OJALIJQ...| B084GRDCHQ|2021-01-16 03:30:17|     2|   Needs some repair|Arts_Crafts_and_S...|\n",
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[✓] Rating distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|rating|   count|\n",
      "+------+--------+\n",
      "|     1|12321453|\n",
      "|     2| 5131627|\n",
      "|     3| 6774232|\n",
      "|     4|11112738|\n",
      "|     5|67191676|\n",
      "+------+--------+\n",
      "\n",
      "\n",
      "[✓] Review counts per category:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       category_name|   count|\n",
      "+--------------------+--------+\n",
      "|Cell_Phones_and_A...|20576383|\n",
      "|          Automotive|19723213|\n",
      "|Patio_Lawn_and_Ga...|16318138|\n",
      "|Grocery_and_Gourm...|14187554|\n",
      "|     Office_Products|12715091|\n",
      "|Arts_Crafts_and_S...| 8876371|\n",
      "|       Baby_Products| 5967954|\n",
      "| Musical_Instruments| 2983780|\n",
      "|          All_Beauty|  694252|\n",
      "|Health_and_Person...|  488990|\n",
      "+--------------------+--------+\n",
      "\n",
      "\n",
      "[✓] Null count summary (subset of key columns):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=====================================================>(135 + 1) / 136]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+----+---------+\n",
      "|user_id|parent_asin|rating|text|timestamp|\n",
      "+-------+-----------+------+----+---------+\n",
      "|      0|          0|     0|   0|        0|\n",
      "+-------+-----------+------+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: Combined Reviews Compact Dataset\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "REV_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\"\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Checking combined reviews parquet at: {REV_COMBINED_COMPACT}\")\n",
    "\n",
    "# Try reading a few files (recursive handles nested folders)\n",
    "df_reviews = (spark.read\n",
    "                  .option(\"recursiveFileLookup\",\"true\")\n",
    "                  .parquet(REV_COMBINED_COMPACT))\n",
    "\n",
    "# --- Basic checks ---\n",
    "print(f\"\\n[✓] Schema:\")\n",
    "df_reviews.printSchema()\n",
    "\n",
    "print(f\"\\n[✓] Total rows (approx): {df_reviews.count():,}\")\n",
    "\n",
    "print(\"\\n[✓] Sample records:\")\n",
    "df_reviews.select(\n",
    "    \"user_id\",\n",
    "    \"parent_asin\",\n",
    "    \"timestamp\",\n",
    "    \"rating\",\n",
    "    \"title\",\n",
    "    \"category_name\"\n",
    ").show(10, truncate=True)\n",
    "\n",
    "# --- Additional quality spot checks ---\n",
    "print(\"\\n[✓] Rating distribution:\")\n",
    "df_reviews.groupBy(\"rating\").count().orderBy(\"rating\").show()\n",
    "\n",
    "print(\"\\n[✓] Review counts per category:\")\n",
    "df_reviews.groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n[✓] Null count summary (subset of key columns):\")\n",
    "df_reviews.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in [\"user_id\",\"parent_asin\",\"rating\",\"text\",\"timestamp\"]\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4d7c40-ad75-4970-851c-03e8464f915d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:05:56] Checking combined meta parquet at: gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\n",
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      " |-- rating_number: long (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- product_image: boolean (nullable = true)\n",
      " |-- product_video: boolean (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Total rows: 5,244,716\n",
      "\n",
      "[✓] Sample rows:\n",
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "|parent_asin|               title|main_category|price|average_rating|rating_number|brand|        store|category_name|\n",
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "| B01AB5SIXO|NuGene NuEye Eye ...|   All Beauty| NULL|           5.0|            1| NULL|         NULL|   All_Beauty|\n",
      "| B07DNP5SY9|18INCH #24 Ash Bl...|   All Beauty| NULL|           1.0|            1| NULL|     benehair|   All_Beauty|\n",
      "| B08F51HG1R|Headbands for Wom...|   All Beauty| NULL|           4.3|           23| NULL|   makersland|   All_Beauty|\n",
      "| B00IIAJYEC|\"THE NASTY\" Mascu...|   All Beauty| NULL|           3.2|           45| NULL| spellboundrx|   All_Beauty|\n",
      "| B07Q8XGVLG|Makeup Blur Remov...|   All Beauty| NULL|           4.4|           24| NULL|  makeup blur|   All_Beauty|\n",
      "| B07VMGV3SK|Kaleidoscope Ther...|   All Beauty| 55.9|           4.5|          978| NULL| kaleidoscope|   All_Beauty|\n",
      "| B01IACTLAY|  Avon Amour 2pc set|   All Beauty| NULL|           5.0|            8| NULL|         avon|   All_Beauty|\n",
      "| B074SYGP1Y|False Magnetic Ey...|   All Beauty| NULL|           2.7|            3| NULL|         NULL|   All_Beauty|\n",
      "| B07R39QGHR|Paraffin Wax Mach...|   All Beauty| NULL|           4.2|           28| NULL|      serfory|   All_Beauty|\n",
      "| B07ZH3JRSW|Professional Eyel...|   All Beauty| NULL|           4.0|           21| NULL|alicrown hair|   All_Beauty|\n",
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[✓] Rows per category_name:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------+\n",
      "|category_name              |count  |\n",
      "+---------------------------+-------+\n",
      "|Cell_Phones_and_Accessories|1288490|\n",
      "|Patio_Lawn_and_Garden      |851907 |\n",
      "|Arts_Crafts_and_Sewing     |801446 |\n",
      "|Office_Products            |710503 |\n",
      "|Grocery_and_Gourmet_Food   |603274 |\n",
      "|Automotive                 |384896 |\n",
      "|Baby_Products              |217724 |\n",
      "|Musical_Instruments        |213593 |\n",
      "|All_Beauty                 |112590 |\n",
      "|Health_and_Personal_Care   |60293  |\n",
      "+---------------------------+-------+\n",
      "\n",
      "\n",
      "[✓] Null counts (key fields):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "|null_parent_asin|null_or_empty_title|null_main_category|null_price|null_avg_rating|null_rating_number|\n",
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "|               0|                372|            317107|   3318499|              0|                 0|\n",
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "\n",
      "\n",
      "[✓] Price summary (exclude 0/negatives for sanity):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+----+---------+-----------------+\n",
      "|percentiles                                          |min |max      |avg              |\n",
      "+-----------------------------------------------------+----+---------+-----------------+\n",
      "|[0.01, 10.15, 16.99, 35.04, 89.99, 500.9937999999989]|0.01|1099995.0|51.25955624217865|\n",
      "+-----------------------------------------------------+----+---------+-----------------+\n",
      "\n",
      "\n",
      "[✓] Average rating stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---+---+-----------------+\n",
      "|percentiles                   |min|max|avg              |\n",
      "+------------------------------+---+---+-----------------+\n",
      "|[1.0, 3.8, 4.3, 4.7, 5.0, 5.0]|1.0|5.0|4.126237702860981|\n",
      "+------------------------------+---+---+-----------------+\n",
      "\n",
      "\n",
      "[✓] Top brands by product count (top 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|brand|count  |\n",
      "+-----+-------+\n",
      "|NULL |5244716|\n",
      "+-----+-------+\n",
      "\n",
      "\n",
      "[✓] Missing main_category by category_name (top 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------+\n",
      "|category_name              |count |\n",
      "+---------------------------+------+\n",
      "|Cell_Phones_and_Accessories|112432|\n",
      "|Arts_Crafts_and_Sewing     |80265 |\n",
      "|Patio_Lawn_and_Garden      |69823 |\n",
      "|Office_Products            |23944 |\n",
      "|Baby_Products              |17880 |\n",
      "|Grocery_and_Gourmet_Food   |7960  |\n",
      "|Musical_Instruments        |3392  |\n",
      "|Automotive                 |1411  |\n",
      "+---------------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: Combined Meta Compact Dataset\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "META_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\"\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Checking combined meta parquet at: {META_COMBINED_COMPACT}\")\n",
    "\n",
    "# Read (recursive in case of nested folder layout)\n",
    "df_meta = (spark.read\n",
    "               .option(\"recursiveFileLookup\",\"true\")\n",
    "               .parquet(META_COMBINED_COMPACT))\n",
    "\n",
    "# --- Basic checks ---\n",
    "print(\"\\n[✓] Schema:\")\n",
    "df_meta.printSchema()\n",
    "\n",
    "print(\"\\n[✓] Total rows:\", f\"{df_meta.count():,}\")\n",
    "\n",
    "print(\"\\n[✓] Sample rows:\")\n",
    "df_meta.select(\n",
    "    \"parent_asin\",\n",
    "    \"title\",\n",
    "    \"main_category\",\n",
    "    \"price\",\n",
    "    \"average_rating\",\n",
    "    \"rating_number\",\n",
    "    \"brand\",\n",
    "    \"store\",\n",
    "    \"category_name\"\n",
    ").show(10, truncate=True)\n",
    "\n",
    "# --- Category coverage ---\n",
    "print(\"\\n[✓] Rows per category_name:\")\n",
    "df_meta.groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# --- Key nulls summary ---\n",
    "print(\"\\n[✓] Null counts (key fields):\")\n",
    "df_meta.select(\n",
    "    F.count(F.when(F.col(\"parent_asin\").isNull(), 1)).alias(\"null_parent_asin\"),\n",
    "    F.count(F.when(F.col(\"title\").isNull() | (F.length(F.col(\"title\")) == 0), 1)).alias(\"null_or_empty_title\"),\n",
    "    F.count(F.when(F.col(\"main_category\").isNull(), 1)).alias(\"null_main_category\"),\n",
    "    F.count(F.when(F.col(\"price\").isNull(), 1)).alias(\"null_price\"),\n",
    "    F.count(F.when(F.col(\"average_rating\").isNull(), 1)).alias(\"null_avg_rating\"),\n",
    "    F.count(F.when(F.col(\"rating_number\").isNull(), 1)).alias(\"null_rating_number\"),\n",
    ").show()\n",
    "\n",
    "# --- Price sanity ---\n",
    "print(\"\\n[✓] Price summary (exclude 0/negatives for sanity):\")\n",
    "df_meta.filter(F.col(\"price\").isNotNull() & (F.col(\"price\") > 0)) \\\n",
    "       .select(\n",
    "           F.expr(\"percentile(price, array(0.0,0.25,0.5,0.75,0.9,0.99))\").alias(\"percentiles\"),\n",
    "           F.min(\"price\").alias(\"min\"),\n",
    "           F.max(\"price\").alias(\"max\"),\n",
    "           F.avg(\"price\").alias(\"avg\")\n",
    "       ).show(truncate=False)\n",
    "\n",
    "# --- Rating sanity ---\n",
    "print(\"\\n[✓] Average rating stats:\")\n",
    "df_meta.filter(F.col(\"average_rating\").isNotNull()) \\\n",
    "       .select(\n",
    "           F.expr(\"percentile(average_rating, array(0.0,0.25,0.5,0.75,0.9,0.99))\").alias(\"percentiles\"),\n",
    "           F.min(\"average_rating\").alias(\"min\"),\n",
    "           F.max(\"average_rating\").alias(\"max\"),\n",
    "           F.avg(\"average_rating\").alias(\"avg\")\n",
    "       ).show(truncate=False)\n",
    "\n",
    "print(\"\\n[✓] Top brands by product count (top 20):\")\n",
    "df_meta.groupBy(\"brand\").count().orderBy(F.desc(\"count\")).show(20, truncate=False)\n",
    "\n",
    "print(\"\\n[✓] Missing main_category by category_name (top 10):\")\n",
    "df_meta.filter(F.col(\"main_category\").isNull()) \\\n",
    "       .groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f3bb1-d264-4e14-9087-4ddcf932cf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}