{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e1f026-4efe-4825-814b-bea95da6b7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08:25] Checking combined meta parquet at: gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      " |-- rating_number: long (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- product_image: boolean (nullable = true)\n",
      " |-- product_video: boolean (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Total rows: 5,244,716\n",
      "\n",
      "[✓] Sample rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "|parent_asin|               title|main_category|price|average_rating|rating_number|brand|        store|category_name|\n",
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "| B01AB5SIXO|NuGene NuEye Eye ...|   All Beauty| NULL|           5.0|            1| NULL|         NULL|   All_Beauty|\n",
      "| B07DNP5SY9|18INCH #24 Ash Bl...|   All Beauty| NULL|           1.0|            1| NULL|     benehair|   All_Beauty|\n",
      "| B08F51HG1R|Headbands for Wom...|   All Beauty| NULL|           4.3|           23| NULL|   makersland|   All_Beauty|\n",
      "| B00IIAJYEC|\"THE NASTY\" Mascu...|   All Beauty| NULL|           3.2|           45| NULL| spellboundrx|   All_Beauty|\n",
      "| B07Q8XGVLG|Makeup Blur Remov...|   All Beauty| NULL|           4.4|           24| NULL|  makeup blur|   All_Beauty|\n",
      "| B07VMGV3SK|Kaleidoscope Ther...|   All Beauty| 55.9|           4.5|          978| NULL| kaleidoscope|   All_Beauty|\n",
      "| B01IACTLAY|  Avon Amour 2pc set|   All Beauty| NULL|           5.0|            8| NULL|         avon|   All_Beauty|\n",
      "| B074SYGP1Y|False Magnetic Ey...|   All Beauty| NULL|           2.7|            3| NULL|         NULL|   All_Beauty|\n",
      "| B07R39QGHR|Paraffin Wax Mach...|   All Beauty| NULL|           4.2|           28| NULL|      serfory|   All_Beauty|\n",
      "| B07ZH3JRSW|Professional Eyel...|   All Beauty| NULL|           4.0|           21| NULL|alicrown hair|   All_Beauty|\n",
      "+-----------+--------------------+-------------+-----+--------------+-------------+-----+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[✓] Rows per category_name:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------+\n",
      "|category_name              |count  |\n",
      "+---------------------------+-------+\n",
      "|Cell_Phones_and_Accessories|1288490|\n",
      "|Patio_Lawn_and_Garden      |851907 |\n",
      "|Arts_Crafts_and_Sewing     |801446 |\n",
      "|Office_Products            |710503 |\n",
      "|Grocery_and_Gourmet_Food   |603274 |\n",
      "|Automotive                 |384896 |\n",
      "|Baby_Products              |217724 |\n",
      "|Musical_Instruments        |213593 |\n",
      "|All_Beauty                 |112590 |\n",
      "|Health_and_Personal_Care   |60293  |\n",
      "+---------------------------+-------+\n",
      "\n",
      "\n",
      "[✓] Null counts (key fields):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "|null_parent_asin|null_or_empty_title|null_main_category|null_price|null_avg_rating|null_rating_number|\n",
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "|               0|                372|            317107|   3318499|              0|                 0|\n",
      "+----------------+-------------------+------------------+----------+---------------+------------------+\n",
      "\n",
      "\n",
      "[✓] Price summary (exclude 0/negatives for sanity):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+----+---------+------------------+\n",
      "|percentiles                                          |min |max      |avg               |\n",
      "+-----------------------------------------------------+----+---------+------------------+\n",
      "|[0.01, 10.15, 16.99, 35.04, 89.99, 500.9937999999989]|0.01|1099995.0|51.259556242178654|\n",
      "+-----------------------------------------------------+----+---------+------------------+\n",
      "\n",
      "\n",
      "[✓] Average rating stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---+---+-----------------+\n",
      "|percentiles                   |min|max|avg              |\n",
      "+------------------------------+---+---+-----------------+\n",
      "|[1.0, 3.8, 4.3, 4.7, 5.0, 5.0]|1.0|5.0|4.126237702860981|\n",
      "+------------------------------+---+---+-----------------+\n",
      "\n",
      "\n",
      "[✓] Top brands by product count (top 20):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|brand|count  |\n",
      "+-----+-------+\n",
      "|NULL |5244716|\n",
      "+-----+-------+\n",
      "\n",
      "\n",
      "[✓] Missing main_category by category_name (top 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------+\n",
      "|category_name              |count |\n",
      "+---------------------------+------+\n",
      "|Cell_Phones_and_Accessories|112432|\n",
      "|Arts_Crafts_and_Sewing     |80265 |\n",
      "|Patio_Lawn_and_Garden      |69823 |\n",
      "|Office_Products            |23944 |\n",
      "|Baby_Products              |17880 |\n",
      "|Grocery_and_Gourmet_Food   |7960  |\n",
      "|Musical_Instruments        |3392  |\n",
      "|Automotive                 |1411  |\n",
      "+---------------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: Combined Meta Compact Dataset\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "META_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\"\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Checking combined meta parquet at: {META_COMBINED_COMPACT}\")\n",
    "\n",
    "# Read (recursive in case of nested folder layout)\n",
    "df_meta = (spark.read\n",
    "               .option(\"recursiveFileLookup\",\"true\")\n",
    "               .parquet(META_COMBINED_COMPACT))\n",
    "\n",
    "# --- Basic checks ---\n",
    "print(\"\\n[✓] Schema:\")\n",
    "df_meta.printSchema()\n",
    "\n",
    "print(\"\\n[✓] Total rows:\", f\"{df_meta.count():,}\")\n",
    "\n",
    "print(\"\\n[✓] Sample rows:\")\n",
    "df_meta.select(\n",
    "    \"parent_asin\",\n",
    "    \"title\",\n",
    "    \"main_category\",\n",
    "    \"price\",\n",
    "    \"average_rating\",\n",
    "    \"rating_number\",\n",
    "    \"brand\",\n",
    "    \"store\",\n",
    "    \"category_name\"\n",
    ").show(10, truncate=True)\n",
    "\n",
    "# --- Category coverage ---\n",
    "print(\"\\n[✓] Rows per category_name:\")\n",
    "df_meta.groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# --- Key nulls summary ---\n",
    "print(\"\\n[✓] Null counts (key fields):\")\n",
    "df_meta.select(\n",
    "    F.count(F.when(F.col(\"parent_asin\").isNull(), 1)).alias(\"null_parent_asin\"),\n",
    "    F.count(F.when(F.col(\"title\").isNull() | (F.length(F.col(\"title\")) == 0), 1)).alias(\"null_or_empty_title\"),\n",
    "    F.count(F.when(F.col(\"main_category\").isNull(), 1)).alias(\"null_main_category\"),\n",
    "    F.count(F.when(F.col(\"price\").isNull(), 1)).alias(\"null_price\"),\n",
    "    F.count(F.when(F.col(\"average_rating\").isNull(), 1)).alias(\"null_avg_rating\"),\n",
    "    F.count(F.when(F.col(\"rating_number\").isNull(), 1)).alias(\"null_rating_number\"),\n",
    ").show()\n",
    "\n",
    "# --- Price sanity ---\n",
    "print(\"\\n[✓] Price summary (exclude 0/negatives for sanity):\")\n",
    "df_meta.filter(F.col(\"price\").isNotNull() & (F.col(\"price\") > 0)) \\\n",
    "       .select(\n",
    "           F.expr(\"percentile(price, array(0.0,0.25,0.5,0.75,0.9,0.99))\").alias(\"percentiles\"),\n",
    "           F.min(\"price\").alias(\"min\"),\n",
    "           F.max(\"price\").alias(\"max\"),\n",
    "           F.avg(\"price\").alias(\"avg\")\n",
    "       ).show(truncate=False)\n",
    "\n",
    "# --- Rating sanity ---\n",
    "print(\"\\n[✓] Average rating stats:\")\n",
    "df_meta.filter(F.col(\"average_rating\").isNotNull()) \\\n",
    "       .select(\n",
    "           F.expr(\"percentile(average_rating, array(0.0,0.25,0.5,0.75,0.9,0.99))\").alias(\"percentiles\"),\n",
    "           F.min(\"average_rating\").alias(\"min\"),\n",
    "           F.max(\"average_rating\").alias(\"max\"),\n",
    "           F.avg(\"average_rating\").alias(\"avg\")\n",
    "       ).show(truncate=False)\n",
    "\n",
    "print(\"\\n[✓] Top brands by product count (top 20):\")\n",
    "df_meta.groupBy(\"brand\").count().orderBy(F.desc(\"count\")).show(20, truncate=False)\n",
    "\n",
    "print(\"\\n[✓] Missing main_category by category_name (top 10):\")\n",
    "df_meta.filter(F.col(\"main_category\").isNull()) \\\n",
    "       .groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa261587-ea19-471e-bff6-3b768e149720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:08:53] Checking combined reviews parquet at: gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\n",
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- helpful_vote: integer (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- review_image: boolean (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Total rows (approx): 102,531,726\n",
      "\n",
      "[✓] Sample records:\n",
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "|             user_id|parent_asin|          timestamp|rating|               title|       category_name|\n",
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "|AGZPNP4EC4Z7CTHY2...| B07V34XSJ8|2021-05-30 08:51:53|     5|        Comfortable!|Arts_Crafts_and_S...|\n",
      "|AFOCCQXZYCTLGLQ4Y...| B0047BITNI|2015-03-10 18:23:14|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AE5XOXRPK5ZCDD2DC...| B08Z7CRNSC|2022-01-17 16:45:32|     5|        Very pleased|Arts_Crafts_and_S...|\n",
      "|AE4JS4KHF5SU7PICZ...| B007C7XPME|2020-11-23 13:29:21|     5|        Fun and Easy|Arts_Crafts_and_S...|\n",
      "|AHZW6N77UGOLTYM6A...| B00FFFR7E2|2020-01-28 07:32:49|     5|        They are big|Arts_Crafts_and_S...|\n",
      "|AEXD6MEZ562LW7JGA...| B005R4FEKA|2018-05-26 02:03:41|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AEK5UPTJQEIOPKJNI...| B071S4747T|2019-08-08 04:24:17|     5|Great product, I ...|Arts_Crafts_and_S...|\n",
      "|AE4SU34EYNA4YQSZ7...| B01LY9ERQC|2016-10-20 15:05:25|     5|          Five Stars|Arts_Crafts_and_S...|\n",
      "|AHORDNMHYEIZEUCTL...| B016HS9EAK|2014-10-07 13:58:30|     4|          Four Stars|Arts_Crafts_and_S...|\n",
      "|AEBG3BZ573OJALIJQ...| B084GRDCHQ|2021-01-16 03:30:17|     2|   Needs some repair|Arts_Crafts_and_S...|\n",
      "+--------------------+-----------+-------------------+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[✓] Rating distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|rating|   count|\n",
      "+------+--------+\n",
      "|     1|12321453|\n",
      "|     2| 5131627|\n",
      "|     3| 6774232|\n",
      "|     4|11112738|\n",
      "|     5|67191676|\n",
      "+------+--------+\n",
      "\n",
      "\n",
      "[✓] Review counts per category:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=====================================================>(135 + 1) / 136]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       category_name|   count|\n",
      "+--------------------+--------+\n",
      "|Cell_Phones_and_A...|20576383|\n",
      "|          Automotive|19723213|\n",
      "|Patio_Lawn_and_Ga...|16318138|\n",
      "|Grocery_and_Gourm...|14187554|\n",
      "|     Office_Products|12715091|\n",
      "|Arts_Crafts_and_S...| 8876371|\n",
      "|       Baby_Products| 5967954|\n",
      "| Musical_Instruments| 2983780|\n",
      "|          All_Beauty|  694252|\n",
      "|Health_and_Person...|  488990|\n",
      "+--------------------+--------+\n",
      "\n",
      "\n",
      "[✓] Null count summary (subset of key columns):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================================>(135 + 1) / 136]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+----+---------+\n",
      "|user_id|parent_asin|rating|text|timestamp|\n",
      "+-------+-----------+------+----+---------+\n",
      "|      0|          0|     0|   0|        0|\n",
      "+-------+-----------+------+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SANITY CHECK: Combined Reviews Compact Dataset\n",
    "# ============================================================\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "REV_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\"\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Checking combined reviews parquet at: {REV_COMBINED_COMPACT}\")\n",
    "\n",
    "# Try reading a few files (recursive handles nested folders)\n",
    "df_reviews = (spark.read\n",
    "                  .option(\"recursiveFileLookup\",\"true\")\n",
    "                  .parquet(REV_COMBINED_COMPACT))\n",
    "\n",
    "# --- Basic checks ---\n",
    "print(f\"\\n[✓] Schema:\")\n",
    "df_reviews.printSchema()\n",
    "\n",
    "print(f\"\\n[✓] Total rows (approx): {df_reviews.count():,}\")\n",
    "\n",
    "print(\"\\n[✓] Sample records:\")\n",
    "df_reviews.select(\n",
    "    \"user_id\",\n",
    "    \"parent_asin\",\n",
    "    \"timestamp\",\n",
    "    \"rating\",\n",
    "    \"title\",\n",
    "    \"category_name\"\n",
    ").show(10, truncate=True)\n",
    "\n",
    "# --- Additional quality spot checks ---\n",
    "print(\"\\n[✓] Rating distribution:\")\n",
    "df_reviews.groupBy(\"rating\").count().orderBy(\"rating\").show()\n",
    "\n",
    "print(\"\\n[✓] Review counts per category:\")\n",
    "df_reviews.groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n[✓] Null count summary (subset of key columns):\")\n",
    "df_reviews.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in [\"user_id\",\"parent_asin\",\"rating\",\"text\",\"timestamp\"]\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abaa6db6-631e-4cfd-b8b0-9656224e5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/05 03:08:20 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/11/05 03:08:20 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/11/05 03:08:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/11/05 03:08:20 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session initialized.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SETUP: Initialize Spark Session for GCS Access\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"MetaSanityCheck\")\n",
    "      .master(\"local[*]\")   # or \"yarn\" / \"cluster\" depending on your setup\n",
    "      .config(\"spark.driver.memory\", \"8g\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "      # ---- GCS connector ----\n",
    "      .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "      .config(\"spark.hadoop.google.cloud.project\", \"qst843-project\")\n",
    "      .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark session initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68632a46-45af-4b29-95f9-6eaaba6e12cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:57:00] Normalizing per-category → gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__normalized\n",
      "[03:57:00] (1/13) All_Beauty  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=All_Beauty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:06] (2/13) Arts_Crafts_and_Sewing  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Arts_Crafts_and_Sewing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:15] (3/13) Automotive  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Automotive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:20] (4/13) Baby_Products  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Baby_Products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 03:57:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@89a28f3[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@b99497c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@b8a59b7]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:25] (5/13) Cell_Phones_and_Accessories  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Cell_Phones_and_Accessories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:34] (6/13) Grocery_and_Gourmet_Food  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Grocery_and_Gourmet_Food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:40] (7/13) Health_and_Personal_Care  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Health_and_Personal_Care\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:45] (8/13) Musical_Instruments  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Musical_Instruments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:50] (9/13) Office_Products  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Office_Products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:57:57] (10/13) Patio_Lawn_and_Garden  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Patio_Lawn_and_Garden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:58:04] (11/13) Pet_Supplies  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Pet_Supplies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:58:10] (12/13) Sports_and_Outdoors  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Sports_and_Outdoors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:58:20] (13/13) Toys_and_Games  ←  gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append/category_name=Toys_and_Games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 03:58:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@13b2353c[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@b3fe08d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@4aac9a3f]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ normalized & wrote\n",
      "[03:58:28] Compaction → gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:59:09] ✅ Done → gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 03:59:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@10aae8c8[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@4a365a09[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@3ef9e04a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:00:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1ba794d6[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@12333434[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@6a2dc269]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize per-category schema and compact (NullType fix) ---\n",
    "import time\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "SRC_BYCAT   = \"gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__append\"\n",
    "NORM_BYCAT  = \"gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_by_cat__normalized\"\n",
    "FINAL_OUT   = \"gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\"\n",
    "\n",
    "CATS = [\n",
    "    \"All_Beauty\",\"Arts_Crafts_and_Sewing\",\"Automotive\",\"Baby_Products\",\n",
    "    \"Cell_Phones_and_Accessories\",\"Grocery_and_Gourmet_Food\",\"Health_and_Personal_Care\",\n",
    "    \"Musical_Instruments\",\"Office_Products\",\"Patio_Lawn_and_Garden\",\n",
    "    \"Pet_Supplies\",\"Sports_and_Outdoors\",\"Toys_and_Games\",\n",
    "]\n",
    "\n",
    "KEEP_COLS = [\n",
    "    \"parent_asin\",\"title\",\"main_category\",\"categories\",\"price\",\n",
    "    \"average_rating\",\"rating_number\",\"store\",\"category_name\"\n",
    "]\n",
    "\n",
    "def normalize_cols(df):\n",
    "    # ensure columns exist\n",
    "    for c in KEEP_COLS:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None))\n",
    "\n",
    "    # price → double (strip currency chars)\n",
    "    price_txt = F.regexp_replace(F.coalesce(F.col(\"price\").cast(\"string\"), F.lit(\"\")), r\"[^0-9eE\\.\\-]+\", \"\")\n",
    "    df = df.withColumn(\"price\", F.when(F.length(price_txt) > 0, price_txt.cast(\"double\")).otherwise(F.lit(None).cast(\"double\")))\n",
    "\n",
    "    # numeric casts\n",
    "    df = df.withColumn(\"average_rating\", F.col(\"average_rating\").cast(\"double\"))\n",
    "    df = df.withColumn(\"rating_number\", F.col(\"rating_number\").cast(\"long\"))\n",
    "\n",
    "    # strings\n",
    "    df = df.withColumn(\"store\", F.lower(F.trim(F.col(\"store\").cast(\"string\"))))\n",
    "    df = df.withColumn(\"title\", F.trim(F.col(\"title\").cast(\"string\")))\n",
    "    df = df.withColumn(\"main_category\", F.trim(F.col(\"main_category\").cast(\"string\")))\n",
    "\n",
    "    # categories → array<string>\n",
    "    if \"categories\" in df.columns:\n",
    "        if not isinstance(df.schema[\"categories\"].dataType, T.ArrayType):\n",
    "            df = df.withColumn(\"categories\", F.array(F.col(\"categories\").cast(\"string\")))\n",
    "        else:\n",
    "            elem = df.schema[\"categories\"].dataType.elementType\n",
    "            if not isinstance(elem, T.StringType):\n",
    "                df = df.withColumn(\"categories\", F.transform(\"categories\", lambda x: x.cast(\"string\")))\n",
    "    else:\n",
    "        df = df.withColumn(\"categories\", F.array(F.lit(None).cast(\"string\")))\n",
    "\n",
    "    return df.select(*KEEP_COLS)\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Normalizing per-category → {NORM_BYCAT}\")\n",
    "\n",
    "for i, cat in enumerate(CATS, 1):\n",
    "    src = f\"{SRC_BYCAT}/category_name={cat}\"\n",
    "    dst_cat_path = f\"{NORM_BYCAT}/category_name={cat}\"   # write into subfolder, no partitionBy needed\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] ({i}/{len(CATS)}) {cat}  ←  {src}\")\n",
    "    try:\n",
    "        df = (spark.read\n",
    "                  .option(\"recursiveFileLookup\",\"true\")\n",
    "                  .parquet(src))\n",
    "\n",
    "        df = normalize_cols(df)\n",
    "\n",
    "        # FORCE non-null, typed category_name for every row\n",
    "        df = df.drop(\"category_name\").withColumn(\"category_name\", F.lit(cat).cast(\"string\"))\n",
    "\n",
    "        # write to the category-specific directory (avoids NullType partition issues)\n",
    "        (df.repartition(8)\n",
    "           .write\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"compression\",\"snappy\")\n",
    "           .parquet(dst_cat_path))\n",
    "        print(\"   ✔ normalized & wrote\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ {cat} failed: {e}\")\n",
    "\n",
    "# Final compaction\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Compaction → {FINAL_OUT}\")\n",
    "df_all = (spark.read\n",
    "              .option(\"recursiveFileLookup\",\"true\")\n",
    "              .parquet(NORM_BYCAT)) \\\n",
    "         .select(*KEEP_COLS)\n",
    "\n",
    "(df_all.repartition(32)\n",
    "      .write.mode(\"overwrite\")\n",
    "      .option(\"compression\",\"snappy\")\n",
    "      .parquet(FINAL_OUT))\n",
    "\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] ✅ Done → {FINAL_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7df47ee-a9b0-4604-b247-ee4336ff0ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MUST be set before any DataFrame is read in this session\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "\n",
    "# (optional but harmless)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"128\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f06d249a-7697-482b-8360-9709786b41a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 03:45:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/11/05 03:45:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/11/05 03:45:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/11/05 03:45:16 INFO SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark revived ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 03:45:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@db577b5[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@11a44ec8[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@537d0064]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n"
     ]
    }
   ],
   "source": [
    "# --- Recreate Spark session with safer memory + scan settings ---\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any half-dead session\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"BronzeMetaUnion-Batched-ByCat\")\n",
    "      .config(\"spark.sql.caseSensitive\", \"true\")          # avoids column name collisions\n",
    "      .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "      .config(\"spark.sql.files.maxPartitionBytes\", 256 * 1024 * 1024)  # 256 MB splits (fewer tasks)\n",
    "      .config(\"spark.sql.files.openCostInBytes\", 4 * 1024 * 1024)      # 4 MB\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "      .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "      # bump memory if you can; otherwise keep these as-is\n",
    "      .config(\"spark.driver.memory\", \"8g\")\n",
    "      .config(\"spark.executor.memory\", \"6g\")\n",
    "      .config(\"spark.driver.memoryOverhead\", \"2g\")\n",
    "      .config(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark revived ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e0c543b-3e32-4d20-855e-908f4ecf1bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:10:36] Checking: gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\n",
      "\n",
      "[✓] Row count:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8215809\n",
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      " |-- rating_number: long (nullable = true)\n",
      " |-- store: string (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n",
      "\n",
      "[✓] Columns check:\n",
      "  missing: []\n",
      "  extra: []\n",
      "\n",
      "[✓] Count by category_name:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[category_name: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[parent_asin_null_ratio: double, title_null_ratio: double, price_null_ratio: double, average_rating_null_ratio: double, rating_number_null_ratio: double, store_null_ratio: double, categories_null_ratio: double, main_category_null_ratio: double, category_name_null_ratio: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Numeric stats (price, average_rating, rating_number):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|price             |average_rating   |rating_number     |\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|count  |3000973           |8215809          |8215809           |\n",
      "|min    |0.0               |1.0              |1                 |\n",
      "|25%    |10.99             |3.8              |4                 |\n",
      "|50%    |18.49             |4.3              |13                |\n",
      "|mean   |49.004546148620946|4.148650425050991|137.08565340309153|\n",
      "|75%    |36.95             |4.7              |53                |\n",
      "|max    |1099995.0         |5.0              |354024            |\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------+\n",
      "|category_name              |count  |\n",
      "+---------------------------+-------+\n",
      "|Sports_and_Outdoors        |1587421|\n",
      "|Cell_Phones_and_Accessories|1288490|\n",
      "|Toys_and_Games             |890874 |\n",
      "|Patio_Lawn_and_Garden      |851907 |\n",
      "|Arts_Crafts_and_Sewing     |801446 |\n",
      "|Office_Products            |710503 |\n",
      "|Grocery_and_Gourmet_Food   |603274 |\n",
      "|Pet_Supplies               |492798 |\n",
      "|Automotive                 |384896 |\n",
      "|Baby_Products              |217724 |\n",
      "|Musical_Instruments        |213593 |\n",
      "|All_Beauty                 |112590 |\n",
      "|Health_and_Personal_Care   |60293  |\n",
      "+---------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 172:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+------------------+-------------------------+------------------------+-------------------+---------------------+------------------------+------------------------+\n",
      "|parent_asin_null_ratio|title_null_ratio|price_null_ratio  |average_rating_null_ratio|rating_number_null_ratio|store_null_ratio   |categories_null_ratio|main_category_null_ratio|category_name_null_ratio|\n",
      "+----------------------+----------------+------------------+-------------------------+------------------------+-------------------+---------------------+------------------------+------------------------+\n",
      "|0.0                   |0.0             |0.6347318931099786|0.0                      |0.0                     |0.01823350567180907|0.0                  |0.07543359394065757     |0.0                     |\n",
      "+----------------------+----------------+------------------+-------------------------+------------------------+-------------------+---------------------+------------------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 04:11:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2f93aa41[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@2d0b5273[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$anon$1@73cbd0f9]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:12:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5ce3ab7f[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@66da3da[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@7182f1ae]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:13:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@636140e5[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@6e10c300[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@508ce6b]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for meta_bronze_combined\n",
    "#PRE CLEANED REVIEWS\n",
    "import time\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "PATH = \"gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\"\n",
    "print(f\"[{time.strftime('%H:%M:%S')}] Checking: {PATH}\")\n",
    "\n",
    "df = (spark.read\n",
    "          .option(\"recursiveFileLookup\",\"true\")\n",
    "          .parquet(PATH))\n",
    "\n",
    "print(\"\\n[✓] Row count:\")\n",
    "print(df.count())\n",
    "\n",
    "print(\"\\n[✓] Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Basic column presence & dtypes\n",
    "expected = {\n",
    "    \"parent_asin\":\"string\",\n",
    "    \"title\":\"string\",\n",
    "    \"main_category\":\"string\",\n",
    "    \"categories\":\"array<string>\",\n",
    "    \"price\":\"double\",\n",
    "    \"average_rating\":\"double\",\n",
    "    \"rating_number\":\"long\",\n",
    "    \"store\":\"string\",\n",
    "    \"category_name\":\"string\",\n",
    "}\n",
    "missing = [c for c in expected if c not in df.columns]\n",
    "extra   = [c for c in df.columns if c not in expected]\n",
    "print(\"\\n[✓] Columns check:\")\n",
    "print(\"  missing:\", missing)\n",
    "print(\"  extra:\", extra)\n",
    "\n",
    "# Quick per-category counts\n",
    "print(\"\\n[✓] Count by category_name:\")\n",
    "display(df.groupBy(\"category_name\").count().orderBy(\"count\", ascending=False))\n",
    "\n",
    "# Null ratios for key fields\n",
    "key_cols = [\"parent_asin\",\"title\",\"price\",\"average_rating\",\"rating_number\",\"store\",\"categories\",\"main_category\",\"category_name\"]\n",
    "nulls = df.select(*[\n",
    "    (F.sum(F.col(c).isNull().cast(\"int\"))/F.count(F.lit(1))).alias(c+\"_null_ratio\") for c in key_cols\n",
    "])\n",
    "display(nulls)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "#PRE CLEANED REVIEWS\n",
    "\n",
    "print(\"\\n[✓] Numeric stats (price, average_rating, rating_number):\")\n",
    "num_stats = (\n",
    "    df.select(\n",
    "        F.col(\"price\").cast(\"double\").alias(\"price\"),\n",
    "        F.col(\"average_rating\").cast(\"double\").alias(\"average_rating\"),\n",
    "        F.col(\"rating_number\").cast(\"long\").alias(\"rating_number\"),\n",
    "    )\n",
    "    .summary(\"count\", \"min\", \"25%\", \"50%\", \"mean\", \"75%\", \"max\")\n",
    ")\n",
    "num_stats.show(truncate=False)\n",
    "\n",
    "# Count by category\n",
    "#PRE CLEANED REVIEWS\n",
    "df.groupBy(\"category_name\").count().orderBy(F.desc(\"count\")).show(50, truncate=False)\n",
    "\n",
    "# Null ratios for key fields\n",
    "key_cols = [\"parent_asin\",\"title\",\"price\",\"average_rating\",\"rating_number\",\"store\",\"categories\",\"main_category\",\"category_name\"]\n",
    "null_exprs = [(F.sum(F.col(c).isNull().cast(\"int\"))/F.count(F.lit(1))).alias(c+\"_null_ratio\") for c in key_cols]\n",
    "df.select(*null_exprs).show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2f98fe1-b86a-46ce-bff4-d08438cd39d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Using BRONZE reviews path: gs://qst843-project/amazon_reviews_2023/bronze/reviews_parquet_by_cat\n",
      "\n",
      "[✓] Row count:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156314842\n",
      "\n",
      "[✓] Schema:\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n",
      "\n",
      "[✓] First 5 rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+-----------+------+------------------------------------------------------------------------------------------+-------------+---------------------------------------+----------------------------+-----------------+----------+\n",
      "|      asin|helpful_vote|images|parent_asin|rating|                                                                                      text|    timestamp|                                  title|                     user_id|verified_purchase|  category|\n",
      "+----------+------------+------+-----------+------+------------------------------------------------------------------------------------------+-------------+---------------------------------------+----------------------------+-----------------+----------+\n",
      "|B01LZA8SGZ|           0|    []| B0BV88374L|   5.0|                           Item came as described! It fit our 2012 Chevy Colorado perfect!|1513092936205|It fit our 2012 Chevy Colorado perfect!|AGXVBIUFLFGMVLATYXHJYL4A5Q7Q|             true|Automotive|\n",
      "|B0B2WGS5ND|           0|    []| B0B2WGS5ND|   5.0|Ease of application process. Clean area with rubbing alcohol.  Make sure that its dry a...|1659124303053|                       Easy to put on!!|AFE337D2J37YRU5U6MVTVKNDKWDA|             true|Automotive|\n",
      "|B00A0GV20Q|           0|    []| B00A0GV20Q|   5.0|Nice quality, framed in silver. I used for wall decoration and worked great. Wish I cou...|1388666735000|                                Perfect|AEVWAM3YWN5URJVJIZZ6XPD2MKIA|             true|Automotive|\n",
      "|B08C27WWVG|           0|    []| B08C27WWVG|   2.0|                  Description said it is waterproof, it is NOT waterproof! Waste of money.|1651801619265|                         Not waterproof|AHITBJSS7KYUBVZPX7M2WJCOIVKQ|             true|Automotive|\n",
      "|B0719J5ZNY|           0|    []| B0719J5ZNY|   5.0|                                                              Looks great on my son’s 370Z|1574097084236|                              Very nice|AHITBJSS7KYUBVZPX7M2WJCOIVKQ|             true|Automotive|\n",
      "+----------+------------+------+-----------+------+------------------------------------------------------------------------------------------+-------------+---------------------------------------+----------------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "[✓] Columns check:\n",
      "  missing: ['category_name']\n",
      "  extra  : ['category']\n",
      "\n",
      "[✓] Numeric summaries (cast to double for safety):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 04:58:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@594ac70c[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@125ac63d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$anon$1@4c744854]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:59:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@3509c25e[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1ffe5174[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$anon$1@5bd10798]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 05:00:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@452b22d[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@3af41154[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$anon$1@57ec828f]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+---------------------+\n",
      "|summary|helpful_vote     |rating           |timestamp            |\n",
      "+-------+-----------------+-----------------+---------------------+\n",
      "|count  |156314842        |156314842        |156314842            |\n",
      "|min    |-4.0             |0.0              |8.85229108E11        |\n",
      "|25%    |0.0              |4.0              |1.492376766E12       |\n",
      "|50%    |0.0              |5.0              |1.573221818128E12    |\n",
      "|mean   |0.877854944829871|4.143757686170326|1.5553575208174023E12|\n",
      "|75%    |0.0              |5.0              |1.624886262304E12    |\n",
      "|max    |41687.0          |5.0              |1.694670041162E12    |\n",
      "+-------+-----------------+-----------------+---------------------+\n",
      "\n",
      "\n",
      "[✓] Null ratios:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 05:01:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@561d4517[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7f328bcb[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$anon$1@65565bc8]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "[Stage 205:====================================================>(101 + 1) / 102]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+---------------+-----------------+----------------+---------------+----------------------------+--------------------+\n",
      "|user_id_null_ratio|parent_asin_null_ratio|asin_null_ratio|rating_null_ratio|title_null_ratio|text_null_ratio|verified_purchase_null_ratio|timestamp_null_ratio|\n",
      "+------------------+----------------------+---------------+-----------------+----------------+---------------+----------------------------+--------------------+\n",
      "|0.0               |0.0                   |0.0            |0.0              |0.0             |0.0            |0.0                         |0.0                 |\n",
      "+------------------+----------------------+---------------+-----------------+----------------+---------------+----------------------------+--------------------+\n",
      "\n",
      "\n",
      "[i] 'category_name' column not found; showing total only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# --- Spark init (safe if a session already exists) ---\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = (SparkSession.getActiveSession()\n",
    "         or SparkSession.builder\n",
    "             .appName(\"BronzeReviewSanityCheck\")\n",
    "             .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "             .getOrCreate())\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# --- Candidate locations (BRONZE = pre-clean) ---\n",
    "BRONZE_ROOT = \"gs://qst843-project/amazon_reviews_2023/bronze\"\n",
    "CANDIDATES = [\n",
    "    f\"{BRONZE_ROOT}/reviews_parquet_by_cat\",   # <— your screenshot path\n",
    "    f\"{BRONZE_ROOT}/review_parquet_by_cat\",\n",
    "    f\"{BRONZE_ROOT}/reviews_parquet\",\n",
    "    f\"{BRONZE_ROOT}/review_parquet\",\n",
    "]\n",
    "\n",
    "def first_existing_parquet(paths):\n",
    "    for p in paths:\n",
    "        try:\n",
    "            # Try to read just 1 row; if it works, the path is good.\n",
    "            _ = (spark.read\n",
    "                    .option(\"recursiveFileLookup\", \"true\")\n",
    "                    .parquet(p)\n",
    "                    .limit(1)\n",
    "                    .count())\n",
    "            return p\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "target = first_existing_parquet(CANDIDATES)\n",
    "if not target:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a bronze reviews parquet folder. \"\n",
    "        \"Checked:\\n  - \" + \"\\n  - \".join(CANDIDATES)\n",
    "    )\n",
    "\n",
    "print(f\"\\n[✓] Using BRONZE reviews path: {target}\")\n",
    "\n",
    "# --- Read full dataset (recursive covers per-category subfolders) ---\n",
    "df = (spark.read\n",
    "          .option(\"recursiveFileLookup\", \"true\")\n",
    "          .parquet(target))\n",
    "\n",
    "# --- Basic info ---\n",
    "print(\"\\n[✓] Row count:\")\n",
    "print(df.count())\n",
    "\n",
    "print(\"\\n[✓] Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n[✓] First 5 rows:\")\n",
    "df.show(5, truncate=90)\n",
    "\n",
    "# --- Column presence check (bronze often has raw fields) ---\n",
    "expected_cols = {\n",
    "    \"user_id\",\"parent_asin\",\"asin\",\"rating\",\"title\",\"text\",\"helpful_vote\",\n",
    "    \"verified_purchase\",\"images\",\"timestamp\",\"category_name\"\n",
    "}\n",
    "present = set(df.columns)\n",
    "print(\"\\n[✓] Columns check:\")\n",
    "print(\"  missing:\", sorted([c for c in expected_cols if c not in present]))\n",
    "print(\"  extra  :\", sorted(list(present - expected_cols)))\n",
    "\n",
    "# --- Numeric summaries (built-in percentiles) ---\n",
    "num_cols = [c for c,t in df.dtypes if t in (\"int\",\"bigint\",\"double\",\"float\",\"long\",\"decimal\")]\n",
    "if num_cols:\n",
    "    print(\"\\n[✓] Numeric summaries (cast to double for safety):\")\n",
    "    (df.select([F.col(c).cast(\"double\").alias(c) for c in num_cols])\n",
    "       .summary(\"count\",\"min\",\"25%\",\"50%\",\"mean\",\"75%\",\"max\")\n",
    "       .show(truncate=False))\n",
    "else:\n",
    "    print(\"\\n[i] No numeric columns detected for summary.\")\n",
    "\n",
    "# --- Null ratios for key columns (guard by existence) ---\n",
    "probe_cols = [c for c in [\"user_id\",\"parent_asin\",\"asin\",\"rating\",\"title\",\"text\",\n",
    "                          \"verified_purchase\",\"timestamp\",\"category_name\"] if c in df.columns]\n",
    "if probe_cols:\n",
    "    null_exprs = [(F.sum(F.col(c).isNull().cast(\"int\"))/F.count(F.lit(1))).alias(f\"{c}_null_ratio\")\n",
    "                  for c in probe_cols]\n",
    "    print(\"\\n[✓] Null ratios:\")\n",
    "    df.select(*null_exprs).show(truncate=False)\n",
    "\n",
    "# --- If per-category folders or partition column present, show counts ---\n",
    "if \"category_name\" in df.columns:\n",
    "    print(\"\\n[✓] Count by category_name (from column):\")\n",
    "    (df.groupBy(\"category_name\").count().orderBy(F.col(\"count\").desc())).show(30, truncate=False)\n",
    "else:\n",
    "    # Fallback: infer from path if no column (grab folder name in the path string)\n",
    "    print(\"\\n[i] 'category_name' column not found; showing total only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f572cbe-3598-44ec-bd35-c43987cb7185",
   "metadata": {},
   "source": [
    "# 🔍 Comparison: Pre-Cleaning vs. Post-Cleaning (Amazon Reviews Dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## 🪙 Pre-Cleaning (BRONZE)\n",
    "\n",
    "* **Path:** `gs://qst843-project/amazon_reviews_2023/bronze/reviews_parquet_by_cat`\n",
    "* **Row Count:** **156,314,842**\n",
    "* **Key Characteristics:**\n",
    "    * **✅ Raw export** directly from the source.\n",
    "    * **🧱 Nested `images` struct** (multiple URLs).\n",
    "    * **⏱️ `timestamp`** as `long` (epoch time).\n",
    "    * **💬 `text` and `title`** are raw, including HTML and line breaks.\n",
    "    * **⚠️ No schema normalization** or type validation.\n",
    "\n",
    "<br>\n",
    "\n",
    "## ⚙️ Post-Cleaning (SILVER)\n",
    "\n",
    "* **Path:** `gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact`\n",
    "* **Row Count:** **102,531,726** (after deduplication and cleaning)\n",
    "* **Transformations Applied:**\n",
    "    * **🧩 Flattened:** `images` replaced with `review_image: boolean`.\n",
    "    * **⏳ Converted:** Epoch timestamp → **Spark `timestamp`** type.\n",
    "    * **🧼 Normalized Text:** Removed HTML artifacts and excessive whitespace.\n",
    "    * **🧠 Standardized:** Renamed `category` → `category_name`, removed redundant `asin`.\n",
    "    * **🧹 Deduplicated** and **⚖️ Harmonized Schema** across all categories.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 🧾 Summary of Changes\n",
    "\n",
    "| Aspect | Pre-Clean (Bronze) | Post-Clean (Silver) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Row Count** | 156,314,842 | **102,531,726** |\n",
    "| **Nested Images** | Complex struct with 4 URL fields | Simplified to **`review_image: boolean`** |\n",
    "| **Timestamp Type** | `Long` (epoch) | **Proper `timestamp` type** |\n",
    "| **Category Field** | `category` | Renamed to **`category_name`** |\n",
    "| **Text Fields** | Raw, unprocessed | **Normalized, cleaned** |\n",
    "| **Asin Handling** | `asin` + `parent_asin` | Only **`parent_asin` retained** |\n",
    "| **Analytics Readiness** | ❌ Raw format | **✅ Fully analysis-ready** |\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Outcome:** The Silver dataset is a fully standardized, analytics-ready version, ensuring type consistency and enabling scalable queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81acc1-f895-47f9-9d24-a72c5058f55b",
   "metadata": {},
   "source": [
    "# 🧩 Comparison: Pre-Cleaning vs. Post-Cleaning (Amazon Metadata Dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Pre-Cleaning (BRONZE — Raw Metadata)\n",
    "\n",
    "* **Path:** `gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined`\n",
    "* **Row Count:** **8,215,809**\n",
    "* **Schema (Raw / Semi-Structured):**\n",
    "    ```text\n",
    "    parent_asin: string\n",
    "    title: string\n",
    "    main_category: string\n",
    "    categories: array<string>\n",
    "    price: double\n",
    "    average_rating: double\n",
    "    rating_number: long\n",
    "    store: string\n",
    "    category_name: string\n",
    "    ```\n",
    "* **🔍 Observations:**\n",
    "    * **⚠️ High Sparsity:** `price_null_ratio ≈ 63.47%`.\n",
    "    * **⚠️ Missing Data:** `main_category_null_ratio ≈ 7.54%`.\n",
    "    * **📊 Raw Stats:** Mean price is high ($49.00) due to outliers/zeros.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Post-Cleaning (SILVER — Compact Metadata)\n",
    "\n",
    "* **Path:** `gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact`\n",
    "* **Row Count:** **5,244,716** (After deduplication and enrichment)\n",
    "* **Schema (Enriched / Standardized / Flattened):**\n",
    "    ```text\n",
    "    parent_asin: string\n",
    "    title: string\n",
    "    main_category: string\n",
    "    categories: array<string>\n",
    "    price: double\n",
    "    features: array<string>\n",
    "    description: array<string>\n",
    "    average_rating: double\n",
    "    rating_number: long\n",
    "    brand: string\n",
    "    store: string\n",
    "    product_image: boolean\n",
    "    product_video: boolean\n",
    "    category_name: string\n",
    "    ```\n",
    "* **🚀 Transformations Applied:**\n",
    "    * **🧩 Added fields:** **`features`**, **`description`**, **`brand`**, **`product_image`**, **`product_video`**.\n",
    "    * **🧹 Deduplication:** Ensured unique `parent_asin` rows (significant row count reduction).\n",
    "    * **🧠 Null Handling:** Preserved but validated required columns (`parent_asin`, `average_rating`, `rating_number` are now guaranteed **non-null**).\n",
    "    * **🔎 Price Summary:** Sanity filtered for meaningful analytics (Average price: **$51.26 USD**).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Summary of Changes\n",
    "\n",
    "| Aspect | Pre-Clean (Bronze) | Post-Clean (Silver) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Row Count** | 8,215,809 | **5,244,716** (deduplicated + enriched) |\n",
    "| **Feature Completeness** | Limited | **+ features, description, brand, media flags** |\n",
    "| **Media Signals** | ❌ None | ✅ **`product_image`**, **`product_video`** |\n",
    "| **Schema Alignment** | Semi-structured, not uniform | **Standardized**, Spark-friendly |\n",
    "| **Metadata Origin** | One row per category parquet | Fully merged into **single compact dataset** |\n",
    "| **Analytics Readiness** | ❌ Mixed, sparse | **✅ Ready for joins** with reviews data |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Outcome\n",
    "\n",
    "The **Silver metadata dataset** is **deduplicated**, **standardized**, and **enriched** with additional product details. It is ready to be seamlessly joined with the Silver reviews dataset via the common key, `parent_asin`.\n",
    "\n",
    "Bottom line: The metadata is now **clean, compact, and purpose-built for analytics and modeling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4533e6-6ece-4f62-a899-cf94c40ef2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/05 04:44:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@662726f2[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7c5a1b9f[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@651f0370]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:45:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@3ab57624[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@b06913e[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@27ace522]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:46:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1a0f4e58[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@657ac736[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@5a4a25c6]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:47:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@33054c87[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@b1a1f6b[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@38ea2a49]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:48:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6173de51[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@5318b931[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@822157e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:49:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@765d59df[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@780bc78c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@6e4c288d]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:50:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@20b55c37[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@9238013[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@2f32011c]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:51:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1609981f[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@3cba6a5f[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@290c46c3]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:52:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@382139ad[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1cd72a0d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@7350435a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:53:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@362b9fe4[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@62b287d6[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@47105ecf]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:54:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@342f410a[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@77634dd[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@718c8fd2]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:55:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7d90dbd4[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@5e112cd4[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@68773b17]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n",
      "25/11/05 04:56:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@35a73f55[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@33f9e27d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@64d91f5a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@234685ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 2]\n"
     ]
    }
   ],
   "source": [
    "# cleaned files are: meta_combined_compact, and reviews_combined_compact\n",
    "# META_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/meta_combined_compact\"\n",
    "# REV_COMBINED_COMPACT = \"gs://qst843-project/amazon_reviews_2023/silver/reviews_combined_compact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ee277-011e-4675-b7c6-61c2af697c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre cleaned are met and review are\n",
    "# PATH = \"gs://qst843-project/amazon_reviews_2023/silver/meta_bronze_combined\"\n",
    "# PATH =\"gs://qst843-project/amazon_reviews_2023/bronze/reviews_parquet_by_cat\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}